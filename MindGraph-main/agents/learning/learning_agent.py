"""
Learning Agent - Intelligent Tutoring System

Uses Qwen LLM to generate contextual questions, validate answers,
provide adaptive hints, and analyze misconceptions.

Phase 2: Intelligent Question Generation + Answer Validation + Adaptive Hints
Phase 3 (Future): Full LangChain agent with prerequisite testing

@author lycosa9527
@made_by MindSpring Team
"""

import logging
from typing import Dict, Any, List, Optional
import json

from agents.main_agent import QwenLLM

logger = logging.getLogger(__name__)


class LearningAgent:
    """
    Intelligent tutoring system agent for Learning Mode.
    
    Capabilities:
    - Generate contextual questions based on node relationships
    - Validate answers with semantic similarity
    - Provide progressive hints (3 levels)
    - Analyze misconceptions
    """
    
    def __init__(self, language: str = 'en'):
        """
        Initialize learning agent.
        
        Args:
            language: 'en' or 'zh'
        """
        self.language = language
        self.llm = QwenLLM(model_type='generation')  # Use generation model
        logger.info(f"[LRNG] LearningAgent initialized | Language: {language}")
    
    def generate_question(
        self,
        node_id: str,
        diagram_type: str,
        spec: Dict[str, Any],
        language: str = 'en'
    ) -> Dict[str, Any]:
        """
        Generate intelligent question for a knocked-out node.
        
        Args:
            node_id: Node ID (e.g., 'attribute_3')
            diagram_type: Type of diagram
            spec: Full diagram specification
            language: 'en' or 'zh'
        
        Returns:
            {
                "node_id": "attribute_3",
                "question": "è¿™ä¸ªä¸'å…‰åˆä½œç”¨'ç›¸è¿çš„ç©ºç™½èŠ‚ç‚¹ä»£è¡¨ä»€ä¹ˆï¼Ÿ",
                "context": {
                    "parent": "å…‰åˆä½œç”¨",
                    "siblings": ["æ°´", "äºŒæ°§åŒ–ç¢³"],
                    "diagram_type": "bubble_map"
                },
                "difficulty": "medium"
            }
        """
        try:
            # Extract context from diagram
            context = self._extract_node_context(node_id, diagram_type, spec)
            
            # Build prompt for question generation
            prompt = self._build_question_prompt(node_id, context, diagram_type, language)
            
            # Generate question with LLM
            response = self.llm._call(prompt)
            
            # Parse response
            question_text = response.strip()
            
            # Determine difficulty based on context
            difficulty = self._assess_difficulty(context, diagram_type)
            
            result = {
                "node_id": node_id,
                "question": question_text,
                "context": context,
                "difficulty": difficulty
            }
            
            logger.info(f"[LRNG] Generated question for {node_id} | {difficulty} | {language}")
            
            return result
            
        except Exception as e:
            logger.error(f"[LRNG] Error generating question: {str(e)}", exc_info=True)
            # Fallback to simple question
            return {
                "node_id": node_id,
                "question": self._get_fallback_question(node_id, language),
                "context": {},
                "difficulty": "easy"
            }
    
    def validate_answer(
        self,
        user_answer: str,
        correct_answer: str,
        question: str,
        context: Dict[str, Any],
        language: str = 'en'
    ) -> Dict[str, Any]:
        """
        Validate user answer with semantic similarity check.
        
        Args:
            user_answer: User's answer
            correct_answer: Correct answer
            question: The question asked
            context: Node context
            language: 'en' or 'zh'
        
        Returns:
            {
                "correct": true/false,
                "confidence": 0.95,
                "message": "...",
                "proceed_to_next": true/false,
                "misconception_analysis": {...}  # If wrong
            }
        """
        try:
            # First check exact match (case-insensitive, trimmed)
            if self._is_exact_match(user_answer, correct_answer):
                return {
                    "correct": True,
                    "confidence": 1.0,
                    "message": self._get_correct_message(language),
                    "proceed_to_next": True
                }
            
            # Use LLM for semantic validation
            is_correct, confidence = self._semantic_validation(
                user_answer, correct_answer, question, context, language
            )
            
            if is_correct:
                return {
                    "correct": True,
                    "confidence": confidence,
                    "message": self._get_correct_message(language),
                    "proceed_to_next": True
                }
            else:
                # Analyze misconception
                misconception = self._analyze_misconception(
                    user_answer, correct_answer, question, context, language
                )
                
                return {
                    "correct": False,
                    "confidence": confidence,
                    "user_answer": user_answer,
                    "correct_answer": correct_answer,
                    "message": self._get_incorrect_message(correct_answer, language),
                    "proceed_to_next": False,
                    "misconception_analysis": misconception
                }
            
        except Exception as e:
            logger.error(f"[LRNG] Error validating answer: {str(e)}", exc_info=True)
            # Fallback to exact match
            is_correct = self._is_exact_match(user_answer, correct_answer)
            return {
                "correct": is_correct,
                "confidence": 1.0 if is_correct else 0.5,
                "message": self._get_correct_message(language) if is_correct else self._get_incorrect_message(correct_answer, language),
                "proceed_to_next": is_correct
            }
    
    def generate_hint(
        self,
        correct_answer: str,
        question: str,
        context: Dict[str, Any],
        hint_level: int = 1,
        language: str = 'en'
    ) -> Dict[str, Any]:
        """
        Generate progressive hint based on level.
        
        Args:
            correct_answer: The correct answer
            question: The question
            context: Node context
            hint_level: 1 (vague) to 3 (explicit)
            language: 'en' or 'zh'
        
        Returns:
            {
                "hint": "...",
                "hint_level": 2,
                "max_hints": 3
            }
        """
        try:
            # Build prompt for hint generation
            prompt = self._build_hint_prompt(correct_answer, question, context, hint_level, language)
            
            # Generate hint with LLM
            response = self.llm._call(prompt)
            
            hint_text = response.strip()
            
            logger.info(f"[LRNG] Generated hint level {hint_level}/3 | {language}")
            
            return {
                "hint": hint_text,
                "hint_level": hint_level,
                "max_hints": 3
            }
            
        except Exception as e:
            logger.error(f"[LRNG] Error generating hint: {str(e)}", exc_info=True)
            # Fallback to simple hint
            return {
                "hint": self._get_fallback_hint(correct_answer, hint_level, language),
                "hint_level": hint_level,
                "max_hints": 3
            }
    
    def verify_understanding(
        self,
        user_answer: str,
        correct_answer: str,
        verification_question: str,
        language: str = 'en'
    ) -> Dict[str, Any]:
        """
        Verify understanding after learning material.
        
        Args:
            user_answer: User's answer to verification question
            correct_answer: Correct answer
            verification_question: The verification question
            language: 'en' or 'zh'
        
        Returns:
            {
                "understanding_verified": true/false,
                "confidence": 0.92,
                "message": "..."
            }
        """
        try:
            # Similar to validate_answer but with understanding focus
            is_correct, confidence = self._semantic_validation(
                user_answer, correct_answer, verification_question, {}, language
            )
            
            if is_correct:
                message = "âœ… " + ("ç†è§£å·²éªŒè¯ï¼ä½ å·²ç»æŒæ¡äº†è¿™ä¸ªæ¦‚å¿µã€‚" if language == 'zh' else "Understanding verified! You've mastered this concept.")
            else:
                message = "âš ï¸ " + ("è®©æˆ‘ä»¬æ¢ä¸ªè§’åº¦å†è¯•è¯•..." if language == 'zh' else "Let's try a different approach...")
            
            return {
                "understanding_verified": is_correct,
                "confidence": confidence,
                "message": message
            }
            
        except Exception as e:
            logger.error(f"[LRNG] Error verifying understanding: {str(e)}", exc_info=True)
            return {
                "understanding_verified": False,
                "confidence": 0.5,
                "message": "Error verifying understanding"
            }
    
    # ========================================================================
    # PRIVATE HELPER METHODS
    # ========================================================================
    
    def _extract_node_context(
        self,
        node_id: str,
        diagram_type: str,
        spec: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Extract contextual information about a node from the diagram.
        
        Returns:
            {
                "parent": "...",
                "siblings": [...],
                "position": "...",
                "diagram_type": "..."
            }
        """
        context = {
            "diagram_type": diagram_type,
            "node_id": node_id
        }
        
        try:
            if diagram_type == 'bubble_map':
                # Extract topic and sibling attributes
                topic = spec.get('topic', 'Central Topic')
                # Handle both string and dict formats
                if isinstance(topic, dict):
                    context['parent'] = topic.get('text', 'Central Topic')
                else:
                    context['parent'] = str(topic)
                
                # Get all attributes
                attributes = spec.get('attributes', [])
                siblings = []
                for attr in attributes:
                    if isinstance(attr, dict):
                        siblings.append(attr.get('text', ''))
                    else:
                        siblings.append(str(attr))
                
                # Remove the current node from siblings
                node_idx = int(node_id.split('_')[-1])
                if node_idx < len(siblings):
                    siblings_filtered = siblings[:node_idx] + siblings[node_idx+1:]
                    context['siblings'] = siblings_filtered
                    context['position'] = f"{node_idx + 1} of {len(attributes)}"
            
            elif diagram_type == 'mind_map':
                # Extract branches and children
                if 'branch' in node_id:
                    # It's a branch
                    main_topic = spec.get('mainTopic', 'Main Topic')
                    if isinstance(main_topic, dict):
                        context['parent'] = main_topic.get('text', 'Main Topic')
                    else:
                        context['parent'] = str(main_topic)
                    branches = spec.get('branches', [])
                    siblings = []
                    for b in branches:
                        if isinstance(b, dict):
                            siblings.append(b.get('text', ''))
                        else:
                            siblings.append(str(b))
                    context['siblings'] = siblings
                elif 'child' in node_id:
                    # It's a child
                    parts = node_id.split('_')
                    if len(parts) >= 3:
                        branch_idx = int(parts[1])
                        branches = spec.get('branches', [])
                        if branch_idx < len(branches):
                            branch = branches[branch_idx]
                            context['parent'] = branch.get('text', 'Branch')
                            children = branch.get('children', [])
                            context['siblings'] = [c.get('text', '') for c in children]
            
            # Add more diagram types as needed
            
        except Exception as e:
            logger.error(f"[LRNG] Error extracting node context: {str(e)}")
        
        return context
    
    def _build_question_prompt(
        self,
        node_id: str,
        context: Dict[str, Any],
        diagram_type: str,
        language: str
    ) -> str:
        """
        Build prompt for generating a contextual question.
        """
        if language == 'zh':
            parent = context.get('parent', 'ä¸»é¢˜')
            siblings = context.get('siblings', [])
            siblings_text = 'ã€'.join(siblings[:3]) if siblings else 'å…¶ä»–èŠ‚ç‚¹'
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•™è‚²ä¸“å®¶ï¼Œæ­£åœ¨å¸®åŠ©å­¦ç”Ÿé€šè¿‡ä¸»åŠ¨å›å¿†æ¥å­¦ä¹ å›¾ç¤ºå†…å®¹ã€‚

å›¾ç¤ºç±»å‹ï¼š{diagram_type}
ä¸­å¿ƒä¸»é¢˜ï¼š{parent}
ç›¸å…³èŠ‚ç‚¹ï¼š{siblings_text}

è¯·ä¸ºä¸€ä¸ªè¢«éšè—çš„èŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­ã€æ¸…æ™°çš„é—®é¢˜ï¼Œå¸®åŠ©å­¦ç”Ÿå›å¿†è¿™ä¸ªèŠ‚ç‚¹çš„å†…å®¹ã€‚

è¦æ±‚ï¼š
1. é—®é¢˜è¦ç®€çŸ­ï¼ˆä¸è¶…è¿‡30ä¸ªå­—ï¼‰
2. æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æç¤ºï¼Œä½†ä¸ç›´æ¥ç»™å‡ºç­”æ¡ˆ
3. ä½¿ç”¨"è¿™ä¸ªèŠ‚ç‚¹"ã€"è¿™ä¸ªä¸...ç›¸è¿çš„èŠ‚ç‚¹"ç­‰è¡¨è¾¾
4. è¯­æ°”å‹å¥½ã€é¼“åŠ±æ€§
5. åªè¾“å‡ºé—®é¢˜æœ¬èº«ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹

é—®é¢˜ï¼š"""
        
        else:  # English
            parent = context.get('parent', 'topic')
            siblings = context.get('siblings', [])
            siblings_text = ', '.join(siblings[:3]) if siblings else 'other nodes'
            
            prompt = f"""You are an educational expert helping students learn through active recall.

Diagram type: {diagram_type}
Central topic: {parent}
Related nodes: {siblings_text}

Generate a short, clear question to help the student recall the hidden node's content.

Requirements:
1. Keep question concise (under 30 words)
2. Provide sufficient context without giving away the answer
3. Use phrases like "this node", "the node connected to..."
4. Friendly, encouraging tone
5. Output ONLY the question, nothing else

Question:"""
        
        return prompt
    
    def _build_hint_prompt(
        self,
        correct_answer: str,
        question: str,
        context: Dict[str, Any],
        hint_level: int,
        language: str
    ) -> str:
        """
        Build prompt for generating progressive hints.
        """
        level_descriptions = {
            1: ("subtle, without giving away much", "å¾®å¦™çš„æç¤ºï¼Œä¸è¦é€éœ²å¤ªå¤š"),
            2: ("clearer hint with some specific clues", "æ›´æ¸…æ™°çš„æç¤ºï¼ŒåŒ…å«ä¸€äº›å…·ä½“çº¿ç´¢"),
            3: ("explicit hint that almost gives the answer", "æ˜ç¡®çš„æç¤ºï¼Œå‡ ä¹ç»™å‡ºç­”æ¡ˆ")
        }
        
        level_desc = level_descriptions[hint_level][1] if language == 'zh' else level_descriptions[hint_level][0]
        
        if language == 'zh':
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•™è‚²ä¸“å®¶ï¼Œæ­£åœ¨ä¸ºå­¦ç”Ÿæä¾›æ¸è¿›å¼æç¤ºã€‚

é—®é¢˜ï¼š{question}
æ­£ç¡®ç­”æ¡ˆï¼š{correct_answer}
å½“å‰æç¤ºçº§åˆ«ï¼š{hint_level}/3

è¯·ç”Ÿæˆä¸€ä¸ª{level_desc}ã€‚

è¦æ±‚ï¼š
1. çº§åˆ«1ï¼šåªæä¾›æ¦‚å¿µç±»åˆ«æˆ–é¢†åŸŸçš„æç¤º
2. çº§åˆ«2ï¼šæä¾›æ›´å…·ä½“çš„ç‰¹å¾æˆ–å…³è”
3. çº§åˆ«3ï¼šå‡ ä¹ç›´æ¥ç‚¹æ˜ç­”æ¡ˆï¼Œä½†ä¸è¦å®Œå…¨è¯´å‡ºæ¥
4. ä¿æŒç®€çŸ­ï¼ˆä¸è¶…è¿‡40ä¸ªå­—ï¼‰
5. è¯­æ°”å‹å¥½ã€é¼“åŠ±
6. åªè¾“å‡ºæç¤ºæœ¬èº«ï¼Œä¸è¦åŒ…å«"æç¤ºï¼š"ç­‰å‰ç¼€

æç¤ºï¼š"""
        
        else:  # English
            prompt = f"""You are an educational expert providing progressive hints to a student.

Question: {question}
Correct answer: {correct_answer}
Current hint level: {hint_level}/3

Generate a {level_desc}.

Requirements:
1. Level 1: Provide category or domain hints only
2. Level 2: Give more specific characteristics or associations
3. Level 3: Almost reveal the answer without saying it directly
4. Keep it concise (under 40 words)
5. Friendly, encouraging tone
6. Output ONLY the hint, no prefix like "Hint:"

Hint:"""
        
        return prompt
    
    def _semantic_validation(
        self,
        user_answer: str,
        correct_answer: str,
        question: str,
        context: Dict[str, Any],
        language: str
    ) -> tuple[bool, float]:
        """
        Use LLM to validate if user answer is semantically correct.
        
        Returns:
            (is_correct, confidence)
        """
        try:
            if language == 'zh':
                prompt = f"""åˆ¤æ–­å­¦ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆåœ¨è¯­ä¹‰ä¸Šä¸€è‡´ã€‚

é—®é¢˜ï¼š{question}
æ ‡å‡†ç­”æ¡ˆï¼š{correct_answer}
å­¦ç”Ÿç­”æ¡ˆï¼š{user_answer}

è¯·åˆ¤æ–­å­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚è€ƒè™‘ï¼š
1. åŒä¹‰è¯ï¼ˆä¾‹å¦‚ï¼š"å¤ªé˜³"="é˜³å…‰"ï¼‰
2. ä¸åŒè¡¨è¾¾æ–¹å¼ï¼ˆä¾‹å¦‚ï¼š"H2O"="æ°´"ï¼‰
3. æ‹¼å†™æˆ–æ ‡ç‚¹çš„å°é”™è¯¯
4. ä½†æ³¨æ„ï¼šæ¦‚å¿µæ€§é”™è¯¯ä¸ç®—æ­£ç¡®ï¼ˆä¾‹å¦‚ï¼š"æ°§æ°”"â‰ "é˜³å…‰"ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ­£ç¡®æ€§ï¼šæ­£ç¡®/é”™è¯¯
ç½®ä¿¡åº¦ï¼š0.0-1.0çš„æ•°å­—
ç†ç”±ï¼šç®€çŸ­è¯´æ˜ï¼ˆä¸€å¥è¯ï¼‰

å›ç­”ï¼š"""
            
            else:  # English
                prompt = f"""Judge if the student's answer is semantically equivalent to the correct answer.

Question: {question}
Correct answer: {correct_answer}
Student answer: {user_answer}

Consider:
1. Synonyms (e.g., "sun" = "sunlight")
2. Different expressions (e.g., "H2O" = "water")
3. Minor spelling or punctuation errors
4. BUT: Conceptual errors don't count (e.g., "oxygen" â‰  "sunlight")

Respond STRICTLY in this format:
Correctness: correct/incorrect
Confidence: number between 0.0-1.0
Reason: brief explanation (one sentence)

Answer:"""
            
            response = self.llm._call(prompt)
            
            # Parse response
            lines = response.strip().split('\n')
            is_correct = False
            confidence = 0.5
            
            for line in lines:
                line = line.strip()
                if ('æ­£ç¡®æ€§' in line or 'Correctness' in line) and 'æ­£ç¡®' in line:
                    is_correct = True
                elif ('ç½®ä¿¡åº¦' in line or 'Confidence' in line):
                    try:
                        confidence = float(line.split(':')[-1].strip())
                    except:
                        confidence = 0.8 if is_correct else 0.5
            
            return is_correct, confidence
            
        except Exception as e:
            logger.error(f"[LRNG] Error in semantic validation: {str(e)}")
            # Fallback to exact match
            return self._is_exact_match(user_answer, correct_answer), 0.5
    
    def _analyze_misconception(
        self,
        user_answer: str,
        correct_answer: str,
        question: str,
        context: Dict[str, Any],
        language: str
    ) -> Dict[str, Any]:
        """
        Analyze what misconception led to the wrong answer.
        """
        try:
            if language == 'zh':
                prompt = f"""åˆ†æå­¦ç”Ÿçš„è¯¯è§£ç±»å‹ã€‚

é—®é¢˜ï¼š{question}
æ­£ç¡®ç­”æ¡ˆï¼š{correct_answer}
å­¦ç”Ÿç­”æ¡ˆï¼š{user_answer}

è¯·åˆ†æå­¦ç”Ÿå¯èƒ½æœ‰ä»€ä¹ˆè¯¯è§£ï¼Œå¹¶æä¾›ç®€çŸ­çš„è¯Šæ–­ã€‚

å›ç­”æ ¼å¼ï¼š
è¯¯è§£ç±»å‹ï¼š[æ··æ·†æ¦‚å¿µ/å› æœå€’ç½®/è®°å¿†é”™è¯¯/å…¶ä»–]
è¯Šæ–­ï¼š[ä¸€å¥è¯è¯´æ˜å­¦ç”Ÿçš„ç†è§£é”™è¯¯]

å›ç­”ï¼š"""
            
            else:  # English
                prompt = f"""Analyze the type of misconception.

Question: {question}
Correct answer: {correct_answer}
Student answer: {user_answer}

Analyze what misconception the student might have and provide a brief diagnosis.

Format:
Misconception type: [concept_confusion/causal_reversal/memory_error/other]
Diagnosis: [one sentence explaining the student's misunderstanding]

Answer:"""
            
            response = self.llm._call(prompt)
            
            # Parse response
            lines = response.strip().split('\n')
            misconception_type = "other"
            diagnosis = response.strip()
            
            for line in lines:
                if 'è¯¯è§£ç±»å‹' in line or 'Misconception type' in line:
                    misconception_type = line.split(':')[-1].strip()
                elif 'è¯Šæ–­' in line or 'Diagnosis' in line:
                    diagnosis = line.split(':')[-1].strip()
            
            return {
                "type": misconception_type,
                "diagnosis": diagnosis,
                "severity": "medium",
                "user_answer": user_answer,
                "correct_answer": correct_answer
            }
            
        except Exception as e:
            logger.error(f"[LRNG] Error analyzing misconception: {str(e)}")
            return {
                "type": "unknown",
                "diagnosis": "Unable to analyze misconception",
                "severity": "medium"
            }
    
    def _assess_difficulty(self, context: Dict[str, Any], diagram_type: str) -> str:
        """
        Assess question difficulty based on context.
        """
        # Simple heuristic for now
        if context.get('siblings') and len(context.get('siblings', [])) > 5:
            return "hard"
        elif context.get('siblings') and len(context.get('siblings', [])) > 2:
            return "medium"
        else:
            return "easy"
    
    def _is_exact_match(self, user_answer: str, correct_answer: str) -> bool:
        """
        Check if answers match exactly (case-insensitive, trimmed).
        """
        return user_answer.lower().strip().replace(' ', '') == correct_answer.lower().strip().replace(' ', '')
    
    def _get_correct_message(self, language: str) -> str:
        """
        Get encouraging message for correct answer.
        """
        if language == 'zh':
            messages = [
                "âœ… å®Œå…¨æ­£ç¡®ï¼",
                "âœ… å¤ªæ£’äº†ï¼",
                "âœ… ä½ ç†è§£å¾—å¾ˆå¥½ï¼",
                "âœ… ç­”å¯¹äº†ï¼"
            ]
        else:
            messages = [
                "âœ… Correct!",
                "âœ… Excellent!",
                "âœ… Well done!",
                "âœ… That's right!"
            ]
        
        import random
        return random.choice(messages)
    
    def _get_incorrect_message(self, correct_answer: str, language: str) -> str:
        """
        Get message for incorrect answer.
        """
        if language == 'zh':
            return f"âŒ ä¸å®Œå…¨æ­£ç¡®ã€‚æ­£ç¡®ç­”æ¡ˆæ˜¯ï¼š{correct_answer}"
        else:
            return f"âŒ Not quite. The correct answer is: {correct_answer}"
    
    def _get_fallback_question(self, node_id: str, language: str) -> str:
        """
        Get simple fallback question if LLM fails.
        """
        if language == 'zh':
            return f"è¯·å¡«å†™èŠ‚ç‚¹ {node_id} çš„å†…å®¹"
        else:
            return f"What is the content of node {node_id}?"
    
    def _get_fallback_hint(self, correct_answer: str, hint_level: int, language: str) -> str:
        """
        Get simple fallback hint if LLM fails.
        """
        if hint_level == 3:
            # Level 3: Give first character and length
            if language == 'zh':
                return f"ğŸ’¡ ç­”æ¡ˆä»¥ã€Œ{correct_answer[0]}ã€å¼€å¤´ï¼Œå…±{len(correct_answer)}ä¸ªå­—ã€‚"
            else:
                return f"ğŸ’¡ The answer starts with '{correct_answer[0]}' and has {len(correct_answer)} characters."
        elif hint_level == 2:
            # Level 2: Give first character
            if language == 'zh':
                return f"ğŸ’¡ ç­”æ¡ˆä»¥ã€Œ{correct_answer[0]}ã€å¼€å¤´ã€‚"
            else:
                return f"ğŸ’¡ The answer starts with '{correct_answer[0]}'."
        else:
            # Level 1: Give category hint
            if language == 'zh':
                return "ğŸ’¡ æƒ³æƒ³å›¾ç¤ºçš„ä¸»é¢˜å’Œè¿™ä¸ªèŠ‚ç‚¹çš„ä½ç½®ã€‚"
            else:
                return "ğŸ’¡ Think about the diagram's theme and this node's position."

