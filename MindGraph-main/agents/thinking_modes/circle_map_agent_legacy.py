"""
Circle Map Thinking Mode Agent (LEGACY)
=========================================

âš ï¸ DEPRECATED: This file is kept for reference only.
Please use the new ReAct-based implementation instead:
â†’ agents/thinking_modes/circle_map_agent_react.py

The new implementation:
- Uses ReAct pattern (Reason â†’ Act â†’ Observe)
- Inherits from BaseThinkingAgent
- Split into smaller files (< 500 lines each)
- Enables diagram-specific behavior
- See docs/THINKGUIDE_REACT_ARCHITECTURE.md

@author lycosa9527
@made_by MindSpring Team
"""

import logging
import json
from enum import Enum
from typing import Dict, AsyncGenerator, Optional

from config.settings import config
from services.llm_service import llm_service
from prompts.thinking_modes.circle_map import (
    CONTEXT_GATHERING_PROMPT_EN,
    CONTEXT_GATHERING_PROMPT_ZH,
    EDUCATIONAL_ANALYSIS_PROMPT_EN,
    EDUCATIONAL_ANALYSIS_PROMPT_ZH,
    ANALYSIS_PROMPT_EN,
    ANALYSIS_PROMPT_ZH,
    REFINEMENT_1_PROMPT_EN,
    REFINEMENT_1_PROMPT_ZH,
    REFINEMENT_2_PROMPT_EN,
    REFINEMENT_2_PROMPT_ZH,
    FINAL_REFINEMENT_PROMPT_EN,
    FINAL_REFINEMENT_PROMPT_ZH,
    EVALUATE_REASONING_PROMPT_EN,
    EVALUATE_REASONING_PROMPT_ZH,
    get_prompt
)

logger = logging.getLogger(__name__)


class CircleMapState(Enum):
    """State machine for Circle Map thinking workflow"""
    CONTEXT_GATHERING = "CONTEXT_GATHERING"
    EDUCATIONAL_ANALYSIS = "EDUCATIONAL_ANALYSIS"
    ANALYSIS = "ANALYSIS"
    REFINEMENT_1 = "REFINEMENT_1"
    REFINEMENT_2 = "REFINEMENT_2"
    FINAL_REFINEMENT = "FINAL_REFINEMENT"
    COMPLETE = "COMPLETE"


class CircleMapThinkingAgent:
    """
    ThinkGuide agent for Circle Maps.
    Uses Socratic method to guide critical thinking.
    """
    
    def __init__(self):
        """Initialize agent with LLM Service"""
        # Use centralized LLM Service
        self.llm = llm_service
        self.model = 'qwen-plus'  # Better reasoning than qwen-turbo
        
        # Session storage (in-memory for MVP)
        self.sessions: Dict[str, Dict] = {}
    
    def _detect_language(self, text: str) -> str:
        """Detect if text is primarily Chinese or English"""
        if not text:
            return 'en'
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        return 'zh' if chinese_chars > len(text) * 0.3 else 'en'
    
    def _get_prompt(self, prompt_name: str, session: Dict, **kwargs) -> str:
        """Get language-appropriate prompt"""
        language = session.get('language', 'en')
        prompt_map = {
            'CONTEXT_GATHERING': (CONTEXT_GATHERING_PROMPT_EN, CONTEXT_GATHERING_PROMPT_ZH),
            'EDUCATIONAL_ANALYSIS': (EDUCATIONAL_ANALYSIS_PROMPT_EN, EDUCATIONAL_ANALYSIS_PROMPT_ZH),
            'ANALYSIS': (ANALYSIS_PROMPT_EN, ANALYSIS_PROMPT_ZH),
            'REFINEMENT_1': (REFINEMENT_1_PROMPT_EN, REFINEMENT_1_PROMPT_ZH),
            'REFINEMENT_2': (REFINEMENT_2_PROMPT_EN, REFINEMENT_2_PROMPT_ZH),
            'FINAL_REFINEMENT': (FINAL_REFINEMENT_PROMPT_EN, FINAL_REFINEMENT_PROMPT_ZH),
            'EVALUATE_REASONING': (EVALUATE_REASONING_PROMPT_EN, EVALUATE_REASONING_PROMPT_ZH),
        }
        prompts = prompt_map.get(prompt_name)
        if not prompts:
            return ""
        prompt = prompts[1] if language == 'zh' else prompts[0]
        return prompt.format(**kwargs)
    
    def _should_suggest_nodes(self, session: Dict, message: str) -> bool:
        """
        Detect if we should suggest diagram nodes.
        Returns True if:
        - User explicitly asks for suggestions/auto-complete
        - Diagram is sparse (< 3 nodes) AND we have gathered sufficient context
        
        NOTE: During CONTEXT_GATHERING, only add nodes if explicitly requested!
        """
        diagram_data = session.get('diagram_data', {})
        children = diagram_data.get('children', [])
        workflow_state = session.get('workflow_state', 'CONTEXT_GATHERING')
        
        # Check if user explicitly requests (always honor explicit requests)
        if message:
            message_lower = message.lower()
            keywords_en = ['auto', 'complete', 'suggest', 'add', 'help me fill', 'generate', 'more nodes']
            keywords_zh = ['è‡ªåŠ¨', 'å®Œæˆ', 'å»ºè®®', 'æ·»åŠ ', 'å¸®æˆ‘', 'ç”Ÿæˆ', 'æ›´å¤šèŠ‚ç‚¹', 'è¡¥å……']
            
            if any(kw in message_lower for kw in keywords_en + keywords_zh):
                return True
        
        # Only auto-suggest for sparse diagrams AFTER context gathering
        # During CONTEXT_GATHERING, we should just ask questions, not force nodes
        if workflow_state != 'CONTEXT_GATHERING' and len(children) < 3:
            return True
        
        return False
    
    async def _generate_suggested_nodes(self, session: Dict) -> list:
        """
        Generate suggested nodes based on center topic and educational context.
        Uses LLM to create meaningful, pedagogically sound suggestions.
        """
        diagram_data = session.get('diagram_data', {})
        center_text = diagram_data.get('center', {}).get('text', '')
        context = session.get('context', {})
        language = session.get('language', 'en')
        
        if language == 'zh':
            prompt = f"""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„K12æ•™è‚²ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹åœ†åœˆå›¾ï¼ˆCircle Mapï¼‰ä¸»é¢˜ç”Ÿæˆ5ä¸ªåˆé€‚çš„è§‚å¯Ÿç‚¹æˆ–å…³è”é¡¹ã€‚

ä¸»é¢˜ï¼š{center_text}

æ•™å­¦èƒŒæ™¯ï¼š
{context.get('raw_message', 'é€šç”¨K12æ•™å­¦')}

è¦æ±‚ï¼š
1. æ¯ä¸ªé¡¹ç›®åº”è¯¥ç®€æ´ï¼ˆ2-6ä¸ªå­—ï¼‰
2. é€‚åˆK12å­¦ç”Ÿç†è§£
3. å…·æœ‰æ•™è‚²æ„ä¹‰
4. æ¶µç›–ä¸åŒè§’åº¦æˆ–æ–¹é¢
5. åªè¾“å‡ºèŠ‚ç‚¹æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·

è¯·ç”Ÿæˆ5ä¸ªèŠ‚ç‚¹ï¼š"""
        else:
            prompt = f"""You are an experienced K12 education expert. Generate 5 appropriate observation points or related items for the following Circle Map topic.

Topic: {center_text}

Educational Context:
{context.get('raw_message', 'General K12 teaching')}

Requirements:
1. Each item should be concise (2-6 words)
2. Appropriate for K12 students
3. Educational value
4. Cover different aspects or angles
5. Output only node text, one per line, no numbering

Generate 5 nodes:"""
        
        try:
            content = await self.llm.chat(
                prompt=prompt,
                model=self.model,
                system_message='You are a helpful K12 education assistant.',
                temperature=0.7,
                max_tokens=200
            )
            
            # Parse response into node list
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            # Remove any numbering (1., 2., etc.)
            nodes = []
            for line in lines:
                # Remove common numbering patterns
                text = line.lstrip('0123456789.-ã€ï¼‰) ')
                if text:
                    nodes.append({
                        'text': text,
                        'position': 'auto'
                    })
            
            logger.info(f"[ThinkGuide] Generated {len(nodes)} suggested nodes: {nodes}")
            return nodes[:5]  # Limit to 5 nodes
            
        except Exception as e:
            logger.error(f"[ThinkGuide] Error generating nodes: {e}")
            return []
    
    async def _detect_user_intent(self, session: Dict, message: str) -> Dict:
        """
        Use LLM to understand user intent and extract structured information.
        Returns a structured intent object.
        
        This is how LangChain agents work - using LLM for intent detection,
        not brittle keyword matching.
        """
        if not message:
            return {'action': 'discuss'}
        
        diagram_data = session.get('diagram_data', {})
        center_text = diagram_data.get('center', {}).get('text', '')
        children = diagram_data.get('children', [])
        language = session.get('language', 'en')
        
        # Build context about current diagram
        nodes_list = '\n'.join([f"{i+1}. {node['text']}" for i, node in enumerate(children)])
        
        workflow_state = session.get('workflow_state', 'CONTEXT_GATHERING')
        
        if language == 'zh':
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«ä¸“å®¶ã€‚åˆ†æç”¨æˆ·çš„æ¶ˆæ¯ï¼Œåˆ¤æ–­ä»–ä»¬æƒ³è¦å¯¹åœ†åœˆå›¾åšä»€ä¹ˆæ“ä½œã€‚

å½“å‰å·¥ä½œæµé˜¶æ®µï¼š{workflow_state}
{'' if workflow_state == 'CONTEXT_GATHERING' else 'ï¼ˆå·²æ”¶é›†åˆ°èƒŒæ™¯ä¿¡æ¯ï¼‰'}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "action": "change_center" | "update_node" | "delete_node" | "update_properties" | "add_nodes" | "discuss",
  "target": "å…·ä½“ç›®æ ‡ï¼ˆä¸»é¢˜åç§°ã€èŠ‚ç‚¹æ–‡æœ¬æˆ–æ–°å±æ€§å€¼ï¼‰",
  "node_index": èŠ‚ç‚¹åºå·ï¼ˆä»…å½“actionä¸ºupdate_nodeã€delete_nodeã€update_propertiesæˆ–update_positionæ—¶ï¼‰,
  "properties": {{
    "fillColor": "é¢œè‰²ä»£ç ï¼ˆå¦‚#FF5722ï¼‰",
    "textColor": "æ–‡å­—é¢œè‰²",
    "strokeColor": "è¾¹æ¡†é¢œè‰²",
    "bold": true/false,
    "italic": true/false,
    "fontSize": æ•°å­—
  }},
  "position": {{
    "angle": è§’åº¦ï¼ˆ0-360ï¼‰ï¼Œ
    "rotate": æ—‹è½¬åº¦æ•°,
    "swap_with": ç›®æ ‡èŠ‚ç‚¹åºå·
  }}
}}

æ“ä½œç±»å‹è¯´æ˜ï¼š
- change_center: ç”¨æˆ·æƒ³æ”¹å˜åœ†åœˆå›¾çš„ä¸­å¿ƒä¸»é¢˜
- update_node: ç”¨æˆ·æƒ³ä¿®æ”¹æŸä¸ªå…·ä½“çš„å¤–å›´èŠ‚ç‚¹æ–‡å­—
- delete_node: ç”¨æˆ·æƒ³åˆ é™¤æŸä¸ªèŠ‚ç‚¹
- update_properties: ç”¨æˆ·æƒ³ä¿®æ”¹èŠ‚ç‚¹çš„è§†è§‰å±æ€§ï¼ˆé¢œè‰²ã€å­—ä½“ç²—ç»†ã€æ–œä½“ç­‰ï¼‰
- update_position: ç”¨æˆ·æƒ³ç§»åŠ¨èŠ‚ç‚¹ä½ç½®ï¼ˆæ—‹è½¬ã€æ¢ä½ç­‰ï¼‰
- add_nodes: ç”¨æˆ·**æ˜ç¡®è¦æ±‚**æ·»åŠ æ–°èŠ‚ç‚¹æˆ–è‡ªåŠ¨è¡¥å……ï¼ˆå¦‚æœåªæ˜¯åˆæ­¥æ²Ÿé€šï¼Œåº”è¯¥é€‰discussï¼‰
- discuss: åªæ˜¯è®¨è®ºã€æä¾›ä¿¡æ¯ã€å›ç­”é—®é¢˜ï¼Œä¸éœ€è¦ä¿®æ”¹å›¾è¡¨

âš ï¸ é‡è¦ï¼šåœ¨CONTEXT_GATHERINGé˜¶æ®µï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¯´"æ·»åŠ èŠ‚ç‚¹"ã€"å¸®æˆ‘ç”Ÿæˆ"ç­‰ï¼Œå¦åˆ™åº”è¯¥è¿”å›"discuss"

ä½ç½®æ§åˆ¶ï¼ˆä»…ç”¨äºupdate_positionï¼‰ï¼š
- "angle": è§’åº¦å€¼ï¼ˆ0-360åº¦ï¼Œé¡ºæ—¶é’ˆæ–¹å‘ï¼Œ0åº¦åœ¨é¡¶éƒ¨ï¼‰
- "rotate": æ—‹è½¬åº¦æ•°ï¼ˆæ­£æ•°é¡ºæ—¶é’ˆï¼Œè´Ÿæ•°é€†æ—¶é’ˆï¼‰
- "swap_with": äº¤æ¢ç›®æ ‡èŠ‚ç‚¹åºå·

é¢œè‰²è¯†åˆ«ï¼š
- çº¢è‰²/çº¢ â†’ #F44336
- è“è‰²/è“ â†’ #2196F3
- ç»¿è‰²/ç»¿ â†’ #4CAF50
- é»„è‰²/é»„ â†’ #FFEB3B
- æ©™è‰²/æ©™ â†’ #FF9800
- ç´«è‰²/ç´« â†’ #9C27B0
- ç²‰è‰²/ç²‰ â†’ #E91E63

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
            
            user_prompt = f"""å½“å‰åœ†åœˆå›¾çŠ¶æ€ï¼š
ä¸­å¿ƒä¸»é¢˜ï¼š{center_text}
å¤–å›´èŠ‚ç‚¹ï¼ˆ{len(children)}ä¸ªï¼‰ï¼š
{nodes_list if nodes_list else 'ï¼ˆæš‚æ— èŠ‚ç‚¹ï¼‰'}

ç”¨æˆ·æ¶ˆæ¯ï¼š{message}"""
        else:
            system_prompt = f"""You are an intent recognition expert. Analyze the user's message to determine what operation they want to perform on the Circle Map.

Current workflow stage: {workflow_state}
{'' if workflow_state == 'CONTEXT_GATHERING' else '(Context already gathered)'}

Return JSON format:
{{
  "action": "change_center" | "update_node" | "delete_node" | "update_properties" | "add_nodes" | "discuss",
  "target": "specific target (topic name, node text, or new property value)",
  "node_index": node number (only for update_node, delete_node, update_properties, or update_position),
  "properties": {{
    "fillColor": "color code (e.g. #FF5722)",
    "textColor": "text color",
    "strokeColor": "border color",
    "bold": true/false,
    "italic": true/false,
    "fontSize": number
  }},
  "position": {{
    "angle": angle value (0-360),
    "rotate": rotation amount,
    "swap_with": target node index
  }}
}}

Action types:
- change_center: User wants to change the center topic
- update_node: User wants to modify a specific outer node's text
- delete_node: User wants to delete a node
- update_properties: User wants to modify node visual properties (color, font weight, italic, etc.)
- update_position: User wants to move/rotate nodes or swap positions
- add_nodes: User **explicitly requests** to add nodes or auto-complete (if just chatting, use 'discuss')
- discuss: Just discussing, providing info, answering questions - no diagram changes needed

âš ï¸ Important: During CONTEXT_GATHERING, unless user explicitly says "add nodes", "generate", etc., return "discuss"

Position control (for update_position only):
- "angle": absolute angle (0-360 degrees, clockwise, 0 is top)
- "rotate": rotation amount (positive = clockwise, negative = counterclockwise)
- "swap_with": swap with target node index

Color recognition:
- red â†’ #F44336
- blue â†’ #2196F3
- green â†’ #4CAF50
- yellow â†’ #FFEB3B
- orange â†’ #FF9800
- purple â†’ #9C27B0
- pink â†’ #E91E63

Return only JSON, no other text."""
            
            user_prompt = f"""Current Circle Map state:
Center topic: {center_text}
Outer nodes ({len(children)} total):
{nodes_list if nodes_list else '(no nodes yet)'}

User message: {message}"""
        
        try:
            # Use LLM Service to understand intent
            result_text = await self.llm.chat(
                prompt=user_prompt,
                model=self.model,
                system_message=system_prompt,
                temperature=0.1,  # Low temperature for consistent intent detection
                max_tokens=500
            )
            
            # Extract JSON (handle markdown code blocks if present)
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0].strip()
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0].strip()
            
            intent = json.loads(result_text)
            
            logger.info(f"[ThinkGuide] LLM detected intent: {intent}")
            return intent
            
        except Exception as e:
            logger.error(f"[ThinkGuide] Error detecting intent: {e}")
            # Fallback to discuss mode
            return {'action': 'discuss'}
    
    async def _check_and_suggest_nodes(
        self,
        session: Dict,
        message: str,
        stage: str
    ) -> AsyncGenerator[Dict, None]:
        """
        Universal helper to check and suggest nodes at ANY workflow stage.
        This makes ThinkGuide truly two-way at all times.
        
        Args:
            session: Current session data
            message: User's message
            stage: Current workflow stage (for logging)
        """
        # Log current diagram state for transparency
        diagram_data = session.get('diagram_data', {})
        current_nodes = len(diagram_data.get('children', []))
        center = diagram_data.get('center', {}).get('text', 'N/A')
        
        logger.info(f"[ThinkGuide-{stage}] Current diagram: '{center}' with {current_nodes} nodes")
        
        # ğŸ†• LLM-BASED INTENT DETECTION (not keyword matching!)
        if message:
            intent = await self._detect_user_intent(session, message)
            action = intent.get('action')
            
            # Handle different actions based on LLM's understanding
            if action == 'change_center':
                new_topic = intent.get('target', '').strip()
                if new_topic:
                    logger.info(f"[ThinkGuide-{stage}] LLM detected topic change: '{center}' â†’ '{new_topic}'")
                    
                    # Generate verbal acknowledgment using LLM for natural language
                    language = session.get('language', 'en')
                    if language == 'zh':
                        acknowledgment_prompt = f"""ç”¨æˆ·æƒ³è¦ä¿®æ”¹åœ†åœˆå›¾ã€‚

å½“å‰ä¸­å¿ƒä¸»é¢˜ï¼š{center}
æ–°ä¸»é¢˜ï¼š{new_topic}

è¯·ç”¨1-2å¥è¯ç¡®è®¤ä½ ç†è§£äº†ç”¨æˆ·çš„æ„å›¾ï¼Œç„¶åè¯´ä½ ä¼šç«‹å³æ›´æ–°ã€‚è¦è‡ªç„¶ã€ç®€æ´ã€‚"""
                    else:
                        acknowledgment_prompt = f"""User wants to modify the Circle Map.

Current center topic: {center}
New topic: {new_topic}

Confirm you understand the user's intent in 1-2 sentences, then say you'll update it. Be natural and concise."""
                    
                    # Stream the acknowledgment
                    async for chunk in self._stream_llm_response(acknowledgment_prompt, session):
                        yield chunk
                    
                    # Now update session diagram data
                    diagram_data['center']['text'] = new_topic
                    
                    # Send diagram update event
                    yield {
                        'event': 'diagram_update',
                        'action': 'update_center',
                        'updates': {
                            'new_text': new_topic
                        }
                    }
                    
                    # Provide completion confirmation using LLM
                    if language == 'zh':
                        completion_prompt = f"""æˆ‘åˆšåˆšæˆåŠŸæ›´æ–°äº†åœ†åœˆå›¾çš„ä¸­å¿ƒä¸»é¢˜ä¸ºã€Œ{new_topic}ã€ã€‚

è¯·ç”¨1-2å¥è¯ï¼š
1. ç¡®è®¤æ›´æ–°å®Œæˆ
2. é¼“åŠ±ç”¨æˆ·ç»§ç»­å®Œå–„è¿™ä¸ªæ–°ä¸»é¢˜çš„åœ†åœˆå›¾

è¦ç®€æ´ã€ç§¯æã€æœ‰æ•™è‚²æ„ä¹‰ã€‚"""
                    else:
                        completion_prompt = f"""I just successfully updated the Circle Map's center topic to "{new_topic}".

In 1-2 sentences:
1. Confirm the update is complete
2. Encourage the user to continue refining this new topic

Be concise, positive, and educational."""
                    
                    async for chunk in self._stream_llm_response(completion_prompt, session):
                        yield chunk
                    
                    logger.info(f"[ThinkGuide-{stage}] Topic updated with verbal confirmation")
                    return
            
            elif action in ('update_node', 'delete_node', 'update_properties', 'update_position'):
                node_index = intent.get('node_index')
                if node_index is not None:
                    node_index = int(node_index) - 1  # Convert to 0-based
                    
                    if 0 <= node_index < len(diagram_data['children']):
                        target_node = diagram_data['children'][node_index]
                        logger.info(f"[ThinkGuide-{stage}] LLM detected {action} for node {node_index+1}: '{target_node.get('text')}'")
                        
                        if action == 'delete_node':
                            language = session.get('language', 'en')
                            old_text = target_node.get('text', '')
                            
                            # Verbal acknowledgment using LLM
                            if language == 'zh':
                                ack_prompt = f"ç”¨æˆ·è¦åˆ é™¤ç¬¬{node_index+1}ä¸ªèŠ‚ç‚¹ã€Œ{old_text}ã€ã€‚ç”¨1å¥è¯ç¡®è®¤ä½ ç†è§£å¹¶ä¼šåˆ é™¤å®ƒã€‚"
                            else:
                                ack_prompt = f"User wants to delete node #{node_index+1}: \"{old_text}\". Confirm in 1 sentence you understand and will remove it."
                            
                            async for chunk in self._stream_llm_response(ack_prompt, session):
                                yield chunk
                            
                            # Delete the node
                            diagram_data['children'] = [n for n in diagram_data['children'] if n['id'] != target_node['id']]
                            
                            yield {
                                'event': 'diagram_update',
                                'action': 'remove_nodes',
                                'updates': {
                                    'node_ids': [target_node['id']]
                                }
                            }
                            
                            # Completion confirmation using LLM
                            if language == 'zh':
                                done_prompt = f"æˆåŠŸåˆ é™¤äº†èŠ‚ç‚¹ã€Œ{old_text}ã€ï¼Œç°åœ¨å›¾ä¸­æœ‰{len(diagram_data['children'])}ä¸ªèŠ‚ç‚¹ã€‚ç”¨1å¥è¯ç®€æ´ç¡®è®¤ã€‚"
                            else:
                                done_prompt = f"Successfully deleted node \"{old_text}\". Now {len(diagram_data['children'])} nodes remain. Confirm briefly in 1 sentence."
                            
                            async for chunk in self._stream_llm_response(done_prompt, session):
                                yield chunk
                            
                            logger.info(f"[ThinkGuide-{stage}] Node deleted with confirmation")
                            return
                        
                        elif action == 'update_node':
                            new_text = intent.get('target', '').strip()
                            if new_text:
                                language = session.get('language', 'en')
                                old_text = target_node.get('text', '')
                                
                                # Verbal acknowledgment using LLM
                                if language == 'zh':
                                    ack_prompt = f"ç”¨æˆ·è¦å°†ç¬¬{node_index+1}ä¸ªèŠ‚ç‚¹ä»ã€Œ{old_text}ã€æ”¹ä¸ºã€Œ{new_text}ã€ã€‚ç”¨1å¥è¯ç¡®è®¤å¹¶è¯´ä¼šæ›´æ–°ã€‚"
                                else:
                                    ack_prompt = f"User wants to change node #{node_index+1} from \"{old_text}\" to \"{new_text}\". Confirm in 1 sentence and say you'll update it."
                                
                                async for chunk in self._stream_llm_response(ack_prompt, session):
                                    yield chunk
                                
                                # Update the node text
                                for node in diagram_data['children']:
                                    if node['id'] == target_node['id']:
                                        node['text'] = new_text
                                        break
                                
                                yield {
                                    'event': 'diagram_update',
                                    'action': 'update_nodes',
                                    'updates': [{
                                        'node_id': target_node['id'],
                                        'new_text': new_text
                                    }]
                                }
                                
                                # Completion confirmation using LLM
                                if language == 'zh':
                                    done_prompt = f"æˆåŠŸæ›´æ–°èŠ‚ç‚¹ä¸ºã€Œ{new_text}ã€ã€‚ç”¨1å¥è¯ç®€æ´ç¡®è®¤ã€‚"
                                else:
                                    done_prompt = f"Successfully updated node to \"{new_text}\". Confirm briefly in 1 sentence."
                                
                                async for chunk in self._stream_llm_response(done_prompt, session):
                                    yield chunk
                                
                                logger.info(f"[ThinkGuide-{stage}] Node updated with confirmation")
                                return
                        
                        elif action == 'update_properties':
                            properties = intent.get('properties', {})
                            if properties:
                                language = session.get('language', 'en')
                                node_text = target_node.get('text', '')
                                
                                # Build description of property changes
                                prop_desc = []
                                if 'fillColor' in properties:
                                    prop_desc.append('é¢œè‰²' if language == 'zh' else 'color')
                                if 'bold' in properties:
                                    prop_desc.append('ç²—ä½“' if language == 'zh' else 'bold')
                                if 'italic' in properties:
                                    prop_desc.append('æ–œä½“' if language == 'zh' else 'italic')
                                
                                props_str = 'ã€'.join(prop_desc) if language == 'zh' else ', '.join(prop_desc)
                                
                                # Verbal acknowledgment using LLM
                                if language == 'zh':
                                    ack_prompt = f"ç”¨æˆ·è¦ä¿®æ”¹ç¬¬{node_index+1}ä¸ªèŠ‚ç‚¹ã€Œ{node_text}ã€çš„{props_str}ã€‚ç”¨1å¥è¯ç¡®è®¤å¹¶è¯´ä¼šæ›´æ–°æ ·å¼ã€‚"
                                else:
                                    ack_prompt = f"User wants to update {props_str} of node #{node_index+1} \"{node_text}\". Confirm in 1 sentence and say you'll update styles."
                                
                                async for chunk in self._stream_llm_response(ack_prompt, session):
                                    yield chunk
                                
                                logger.info(f"[ThinkGuide-{stage}] Updating properties: {properties}")
                                
                                yield {
                                    'event': 'diagram_update',
                                    'action': 'update_properties',
                                    'updates': [{
                                        'node_id': target_node['id'],
                                        'properties': properties
                                    }]
                                }
                                
                                # Completion confirmation using LLM
                                if language == 'zh':
                                    done_prompt = "æˆåŠŸæ›´æ–°èŠ‚ç‚¹æ ·å¼ã€‚ç”¨1å¥è¯ç®€æ´ç¡®è®¤ã€‚"
                                else:
                                    done_prompt = "Successfully updated node styles. Confirm briefly in 1 sentence."
                                
                                async for chunk in self._stream_llm_response(done_prompt, session):
                                    yield chunk
                                
                                logger.info(f"[ThinkGuide-{stage}] Properties updated with confirmation")
                                return
                        
                        elif action == 'update_position':
                            position = intent.get('position', {})
                            if position:
                                language = session.get('language', 'en')
                                node_text = target_node.get('text', '')
                                
                                # Handle swap operation
                                if 'swap_with' in position:
                                    swap_index = int(position['swap_with']) - 1  # Convert to 0-based
                                    if 0 <= swap_index < len(diagram_data['children']) and swap_index != node_index:
                                        swap_node = diagram_data['children'][swap_index]
                                        swap_text = swap_node.get('text', '')
                                        
                                        # Verbal acknowledgment using LLM
                                        if language == 'zh':
                                            ack_prompt = f"ç”¨æˆ·è¦äº¤æ¢ç¬¬{node_index+1}ä¸ªèŠ‚ç‚¹ã€Œ{node_text}ã€å’Œç¬¬{swap_index+1}ä¸ªèŠ‚ç‚¹ã€Œ{swap_text}ã€çš„ä½ç½®ã€‚ç”¨1å¥è¯ç¡®è®¤ã€‚"
                                        else:
                                            ack_prompt = f"User wants to swap positions of node #{node_index+1} \"{node_text}\" and node #{swap_index+1} \"{swap_text}\". Confirm in 1 sentence."
                                        
                                        async for chunk in self._stream_llm_response(ack_prompt, session):
                                            yield chunk
                                        
                                        yield {
                                            'event': 'diagram_update',
                                            'action': 'swap_positions',
                                            'updates': {
                                                'node1_id': target_node['id'],
                                                'node2_id': swap_node['id']
                                            }
                                        }
                                        
                                        # Completion confirmation using LLM
                                        if language == 'zh':
                                            done_prompt = "æˆåŠŸäº¤æ¢ä¸¤ä¸ªèŠ‚ç‚¹çš„ä½ç½®ã€‚ç”¨1å¥è¯ç¡®è®¤ã€‚"
                                        else:
                                            done_prompt = "Successfully swapped node positions. Confirm in 1 sentence."
                                        
                                        async for chunk in self._stream_llm_response(done_prompt, session):
                                            yield chunk
                                        
                                        logger.info(f"[ThinkGuide-{stage}] Swapped positions with confirmation")
                                        return
                                
                                # Handle angle/rotation operations
                                logger.info(f"[ThinkGuide-{stage}] Updating position: {position}")
                                
                                # Verbal acknowledgment using LLM
                                if language == 'zh':
                                    ack_prompt = f"ç”¨æˆ·è¦è°ƒæ•´ç¬¬{node_index+1}ä¸ªèŠ‚ç‚¹ã€Œ{node_text}ã€çš„ä½ç½®ã€‚ç”¨1å¥è¯ç¡®è®¤ã€‚"
                                else:
                                    ack_prompt = f"User wants to adjust position of node #{node_index+1} \"{node_text}\". Confirm in 1 sentence."
                                
                                async for chunk in self._stream_llm_response(ack_prompt, session):
                                    yield chunk
                                
                                yield {
                                    'event': 'diagram_update',
                                    'action': 'update_position',
                                    'updates': [{
                                        'node_id': target_node['id'],
                                        'node_index': node_index,
                                        'position': position
                                    }]
                                }
                                
                                # Completion confirmation using LLM
                                if language == 'zh':
                                    done_prompt = "æˆåŠŸè°ƒæ•´èŠ‚ç‚¹ä½ç½®ã€‚ç”¨1å¥è¯ç¡®è®¤ã€‚"
                                else:
                                    done_prompt = "Successfully adjusted node position. Confirm in 1 sentence."
                                
                                async for chunk in self._stream_llm_response(done_prompt, session):
                                    yield chunk
                                
                                logger.info(f"[ThinkGuide-{stage}] Position updated with confirmation")
                                return
            
            elif action == 'discuss':
                # User just wants to discuss, don't modify diagram
                logger.info(f"[ThinkGuide-{stage}] LLM detected discussion-only intent, no diagram changes")
                return
        
        # Check if we should suggest nodes (bulk operation)
        if self._should_suggest_nodes(session, message):
            logger.info(f"[ThinkGuide-{stage}] Triggering node generation (sparse diagram or user request)")
            
            language = session.get('language', 'en')
            center = diagram_data.get('center', {}).get('text', '')
            
            # Verbal acknowledgment using LLM
            if language == 'zh':
                ack_prompt = f"ç”¨æˆ·å¸Œæœ›ä¸ºä¸»é¢˜ã€Œ{center}ã€æ·»åŠ èŠ‚ç‚¹ã€‚ç”¨1-2å¥è¯è¯´ä½ ä¼šæ€è€ƒç›¸å…³æ¦‚å¿µå¹¶æ·»åŠ ã€‚"
            else:
                ack_prompt = f"User wants to add nodes for topic \"{center}\". Say in 1-2 sentences you'll think about relevant concepts and add them."
            
            async for chunk in self._stream_llm_response(ack_prompt, session):
                yield chunk
            
            suggested_nodes = await self._generate_suggested_nodes(session)
            
            if suggested_nodes:
                logger.info(f"[ThinkGuide-{stage}] Sending {len(suggested_nodes)} node suggestions to frontend")
                
                # List the nodes being added
                node_list = ', '.join([f"ã€Œ{n['text']}ã€" for n in suggested_nodes]) if language == 'zh' else ', '.join([f"\"{n['text']}\"" for n in suggested_nodes])
                
                if language == 'zh':
                    adding_prompt = f"æˆ‘å»ºè®®æ·»åŠ è¿™{len(suggested_nodes)}ä¸ªèŠ‚ç‚¹ï¼š{node_list}ã€‚ç”¨1å¥è¯è¯´æ˜å¹¶è¯´ä¼šæ·»åŠ åˆ°å›¾ä¸­ã€‚"
                else:
                    adding_prompt = f"I suggest these {len(suggested_nodes)} nodes: {node_list}. Say in 1 sentence you'll add them to the diagram."
                
                async for chunk in self._stream_llm_response(adding_prompt, session):
                    yield chunk
                
                # Update session diagram data optimistically
                # (Frontend will send back actual state on next message)
                diagram_data.setdefault('children', []).extend(suggested_nodes)
                session['node_count'] = len(diagram_data['children'])
                
                yield {
                    'event': 'diagram_update',
                    'action': 'add_nodes',
                    'updates': suggested_nodes
                }
                
                # Completion confirmation using LLM
                if language == 'zh':
                    done_prompt = f"æˆåŠŸæ·»åŠ {len(suggested_nodes)}ä¸ªèŠ‚ç‚¹ï¼Œç°åœ¨å…±{session['node_count']}ä¸ªã€‚ç”¨1-2å¥è¯ç¡®è®¤å¹¶é¼“åŠ±ç»§ç»­å®Œå–„åœ†åœˆå›¾ã€‚"
                else:
                    done_prompt = f"Successfully added {len(suggested_nodes)} nodes. Now {session['node_count']} total. Confirm in 1-2 sentences and encourage refining the Circle Map."
                
                async for chunk in self._stream_llm_response(done_prompt, session):
                    yield chunk
                
                logger.info(f"[ThinkGuide-{stage}] Diagram updated with confirmation: {current_nodes} â†’ {session['node_count']} nodes")
    
    async def process_step(
        self,
        message: str,
        session_id: str,
        diagram_data: Dict,
        current_state: str,
        user_id: str = None
    ) -> AsyncGenerator[Dict, None]:
        """
        Main entry point - processes one step of the workflow.
        Yields SSE events.
        """
        
        # Get or create session
        if session_id not in self.sessions:
            # Detect language from diagram data
            center_text = diagram_data.get('center', {}).get('text', '')
            children_count = len(diagram_data.get('children', []))
            detected_language = self._detect_language(center_text)
            
            logger.info(f"[ThinkGuide] Creating session with diagram - Center: '{center_text}' | Children: {children_count}")
            
            self.sessions[session_id] = {
                'session_id': session_id,
                'user_id': user_id,
                'state': CircleMapState.CONTEXT_GATHERING,
                'diagram_data': diagram_data,
                'language': detected_language,
                'history': [],
                'context': {},
                'node_count': children_count,
                'node_learning_material': {}  # For hover tooltips!
            }
            logger.info(f"[ThinkGuide] New session: {session_id} | Language: {detected_language}")
        
        session = self.sessions[session_id]
        
        # Update diagram data if provided
        if diagram_data:
            session['diagram_data'] = diagram_data
        
        # Route to appropriate handler based on state
        try:
            state = CircleMapState(current_state)
        except ValueError:
            logger.error(f"[ThinkGuide] Invalid state: {current_state}")
            yield {
                'event': 'error',
                'message': f'Invalid state: {current_state}'
            }
            return
        
        logger.info(f"[ThinkGuide] Processing state: {state.value} | Session: {session_id}")
        
        # Route to state handler
        if state == CircleMapState.CONTEXT_GATHERING:
            async for chunk in self._handle_context_gathering(session, message):
                yield chunk
        
        elif state == CircleMapState.EDUCATIONAL_ANALYSIS:
            async for chunk in self._handle_educational_analysis(session, message):
                yield chunk
        
        elif state == CircleMapState.ANALYSIS:
            async for chunk in self._handle_analysis(session, message):
                yield chunk
        
        elif state == CircleMapState.REFINEMENT_1:
            async for chunk in self._handle_refinement_1(session, message):
                yield chunk
        
        elif state == CircleMapState.REFINEMENT_2:
            async for chunk in self._handle_refinement_2(session, message):
                yield chunk
        
        elif state == CircleMapState.FINAL_REFINEMENT:
            async for chunk in self._handle_final_refinement(session, message):
                yield chunk
        
        else:
            yield {
                'event': 'error',
                'message': f'Unknown state: {state}'
            }
    
    async def _handle_context_gathering(
        self,
        session: Dict,
        message: str
    ) -> AsyncGenerator[Dict, None]:
        """Step 1: Gather educational context"""
        
        if not message:
            # First time - ask for context
            # âŒ NEVER auto-generate nodes on initialization!
            # Only ask questions and wait for user input
            prompt = self._get_prompt(
                'CONTEXT_GATHERING',
                session,
                center_node=session['diagram_data']['center']['text']
            )
            
            async for chunk in self._stream_llm_response(prompt, session):
                yield chunk
            
            # No diagram modifications on init - wait for user's first message!
        
        else:
            # User provided context - store it directly
            session['history'].append({
                'role': 'user',
                'content': message,
                'state': 'CONTEXT_GATHERING'
            })
            
            # Store the raw context message
            # The LLM prompts will use this context naturally in their templates
            session['context'] = {
                'raw_message': message,
                # For template compatibility, include message as primary field
                'grade_level': message,
                'objective': message,
                'lesson_context': message,
                'subject': message
            }
            
            logger.info(f"[ThinkGuide] Stored context: {message[:100]}...")
            
            # ğŸ†• Check if user wants to modify diagram BEFORE transitioning
            # This is critical - the message gets lost after state transition!
            async for chunk in self._check_and_suggest_nodes(session, message, 'CONTEXT_GATHERING'):
                yield chunk
            
            # Transition to EDUCATIONAL_ANALYSIS
            session['state'] = CircleMapState.EDUCATIONAL_ANALYSIS
            
            yield {
                'event': 'state_transition',
                'new_state': 'EDUCATIONAL_ANALYSIS',
                'progress': 25
            }
            
            # Automatically start educational analysis
            async for chunk in self._handle_educational_analysis(session, ''):
                yield chunk
    
    async def _handle_educational_analysis(
        self,
        session: Dict,
        message: str
    ) -> AsyncGenerator[Dict, None]:
        """Step 2: Provide educational content about each node"""
        
        # ğŸ†• TWO-WAY: Check if user wants to modify diagram at this stage
        async for chunk in self._check_and_suggest_nodes(session, message, 'EDUCATIONAL_ANALYSIS'):
            yield chunk
        
        nodes = session['diagram_data']['children']
        context = session['context']
        
        # Build educational analysis prompt
        prompt = self._get_prompt(
            'EDUCATIONAL_ANALYSIS',
            session,
            center_node=session['diagram_data']['center']['text'],
            nodes='\n'.join([f"- {node['text']}" for node in nodes]),
            node_count=len(nodes),
            grade_level=context.get('grade_level', 'not specified'),
            objective=context.get('objective', 'not specified')
        )
        
        # Stream educational content
        full_response = ""
        async for chunk in self._stream_llm_response(prompt, session):
            if chunk.get('event') == 'message_chunk':
                full_response += chunk.get('content', '')
            yield chunk
        
        # Store learning material per node (simplified for MVP)
        # In production, parse response to extract per-node content
        for node in nodes:
            session['node_learning_material'][node.get('id', node.get('text'))] = {
                'node_name': node['text'],
                'full_analysis': full_response
            }
        
        # Transition to ANALYSIS (Socratic questions)
        session['state'] = CircleMapState.ANALYSIS
        
        yield {
            'event': 'state_transition',
            'new_state': 'ANALYSIS',
            'progress': 40
        }
        
        # Automatically start Socratic analysis
        async for chunk in self._handle_analysis(session, ''):
            yield chunk
    
    async def _handle_analysis(
        self,
        session: Dict,
        message: str
    ) -> AsyncGenerator[Dict, None]:
        """Step 3: Socratic questioning about nodes"""
        
        # ğŸ†• TWO-WAY: Check if user wants to modify diagram at this stage
        async for chunk in self._check_and_suggest_nodes(session, message, 'ANALYSIS'):
            yield chunk
        
        nodes = session['diagram_data']['children']
        context = session['context']
        
        prompt = self._get_prompt(
            'ANALYSIS',
            session,
            center_node=session['diagram_data']['center']['text'],
            nodes=', '.join([node['text'] for node in nodes]),
            node_count=len(nodes),
            grade_level=context.get('grade_level', 'not specified'),
            objective=context.get('objective', 'not specified')
        )
        
        async for chunk in self._stream_llm_response(prompt, session):
            yield chunk
        
        # Transition to REFINEMENT_1
        session['state'] = CircleMapState.REFINEMENT_1
        
        yield {
            'event': 'state_transition',
            'new_state': 'REFINEMENT_1',
            'progress': 60
        }
        
        # Ask refinement question
        refinement_prompt = self._get_prompt(
            'REFINEMENT_1',
            session,
            node_count=len(nodes),
            grade_level=context.get('grade_level', ''),
            objective=context.get('objective', ''),
            removals=len(nodes) - 8
        )
        
        async for chunk in self._stream_llm_response(refinement_prompt, session):
            yield chunk
    
    async def _handle_refinement_1(
        self,
        session: Dict,
        message: str
    ) -> AsyncGenerator[Dict, None]:
        """Step 4: First refinement (N â†’ 8)"""
        
        # ğŸ†• TWO-WAY: Check if user wants to modify diagram at this stage
        async for chunk in self._check_and_suggest_nodes(session, message, 'REFINEMENT_1'):
            yield chunk
        
        session['history'].append({
            'role': 'user',
            'content': message,
            'state': 'REFINEMENT_1'
        })
        
        # Evaluate reasoning
        evaluation_prompt = self._get_prompt(
            'EVALUATE_REASONING',
            session,
            removed_nodes='extracted from message',
            user_reasoning=message,
            center_node=session['diagram_data']['center']['text'],
            objective=session['context'].get('objective', '')
        )
        
        async for chunk in self._stream_llm_response(evaluation_prompt, session):
            yield chunk
        
        # Update node count
        session['node_count'] = 8
        
        # Transition to REFINEMENT_2
        session['state'] = CircleMapState.REFINEMENT_2
        
        yield {
            'event': 'state_transition',
            'new_state': 'REFINEMENT_2',
            'progress': 75
        }
        
        # Ask next refinement
        refinement_prompt = self._get_prompt(
            'REFINEMENT_2',
            session,
            grade_level=session['context'].get('grade_level', '')
        )
        
        async for chunk in self._stream_llm_response(refinement_prompt, session):
            yield chunk
    
    async def _handle_refinement_2(
        self,
        session: Dict,
        message: str
    ) -> AsyncGenerator[Dict, None]:
        """Step 5: Second refinement (8 â†’ 6)"""
        
        # ğŸ†• TWO-WAY: Check if user wants to modify diagram at this stage
        async for chunk in self._check_and_suggest_nodes(session, message, 'REFINEMENT_2'):
            yield chunk
        
        session['history'].append({
            'role': 'user',
            'content': message,
            'state': 'REFINEMENT_2'
        })
        
        # Evaluate reasoning
        evaluation_prompt = self._get_prompt(
            'EVALUATE_REASONING',
            session,
            removed_nodes='extracted from message',
            user_reasoning=message,
            center_node=session['diagram_data']['center']['text'],
            objective=session['context'].get('objective', '')
        )
        
        async for chunk in self._stream_llm_response(evaluation_prompt, session):
            yield chunk
        
        session['node_count'] = 6
        
        # Transition to FINAL_REFINEMENT
        session['state'] = CircleMapState.FINAL_REFINEMENT
        
        yield {
            'event': 'state_transition',
            'new_state': 'FINAL_REFINEMENT',
            'progress': 90
        }
        
        # Ask final refinement
        final_prompt = self._get_prompt(
            'FINAL_REFINEMENT',
            session,
            center_node=session['diagram_data']['center']['text']
        )
        
        async for chunk in self._stream_llm_response(final_prompt, session):
            yield chunk
    
    async def _handle_final_refinement(
        self,
        session: Dict,
        message: str
    ) -> AsyncGenerator[Dict, None]:
        """Step 6: Final refinement (6 â†’ 5)"""
        
        # ğŸ†• TWO-WAY: Check if user wants to modify diagram at this stage
        async for chunk in self._check_and_suggest_nodes(session, message, 'FINAL_REFINEMENT'):
            yield chunk
        
        session['history'].append({
            'role': 'user',
            'content': message,
            'state': 'FINAL_REFINEMENT'
        })
        
        # Final evaluation and completion
        completion_prompt = f"""
The teacher's final decision: {message}

Acknowledge their deep thinking and provide a brief summary:
1. The 5 core nodes they've identified
2. The thinking process they demonstrated
3. How this refined map serves their educational objective

End with encouragement about their critical thinking journey.
"""
        
        async for chunk in self._stream_llm_response(completion_prompt, session):
            yield chunk
        
        # Mark complete
        session['state'] = CircleMapState.COMPLETE
        
        yield {
            'event': 'state_transition',
            'new_state': 'COMPLETE',
            'progress': 100
        }
        
        yield {
            'event': 'complete',
            'summary': {
                'final_node_count': 5,
                'history': session['history']
            }
        }
    
    async def _stream_static_message(
        self,
        message: str,
        session: Dict
    ) -> AsyncGenerator[Dict, None]:
        """
        Stream a static message as chunks (no LLM call).
        Used for confirmations and system messages.
        """
        # Simulate streaming by splitting into words
        words = message.split(' ')
        full_content = ""
        
        for i, word in enumerate(words):
            chunk_text = word if i == 0 else f" {word}"
            full_content += chunk_text
            
            yield {
                'event': 'message_chunk',
                'content': chunk_text
            }
            
            # Small delay for natural typing effect
            import asyncio
            await asyncio.sleep(0.02)
        
        # Store in history
        session['history'].append({
            'role': 'assistant',
            'content': full_content,
            'state': session['state'].value if hasattr(session.get('state'), 'value') else 'UNKNOWN'
        })
        
        yield {
            'event': 'message_complete',
            'full_content': full_content
        }
    
    async def _stream_llm_response(
        self,
        prompt: str,
        session: Dict
    ) -> AsyncGenerator[Dict, None]:
        """Helper: Stream LLM response as SSE chunks (for actual prompts)"""
        
        try:
            full_content = ""
            
            # Prepare messages with system instruction for concise, professional responses
            language = session.get('language', 'en')
            if language == 'zh':
                system_msg = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ€ç»´æ•™å­¦ä¸“å®¶ï¼ˆTeaching Thinking Professionalï¼‰ã€‚

ä½ çš„è§’è‰²ï¼š
- å¸®åŠ©æ•™å¸ˆé€šè¿‡è‹æ ¼æ‹‰åº•å¼æé—®æ·±åŒ–æ€è€ƒ
- å¼•å¯¼æ•™å¸ˆå‘ç°æ¦‚å¿µçš„æœ¬è´¨å’Œä¼˜å…ˆçº§
- åŸ¹å…»æ‰¹åˆ¤æ€§æ€ç»´å’Œæ•™å­¦è®¾è®¡èƒ½åŠ›

ä½ çš„é£æ ¼ï¼š
- ç®€æ´ã€æ¸…æ™°ã€ä¸“ä¸š
- ä¸ä½¿ç”¨è¡¨æƒ…ç¬¦å·
- ç›´æ¥ã€æœ‰é’ˆå¯¹æ€§
- æé—®è€Œéè¯´æ•™"""
            else:
                system_msg = """You are a Teaching Thinking Professional.

Your role:
- Help teachers deepen thinking through Socratic questioning
- Guide teachers to discover essence and priorities of concepts
- Develop critical thinking and instructional design skills

Your style:
- Concise, clear, professional
- No emojis
- Direct and targeted
- Ask, don't lecture"""
            
            # Stream from LLM Service
            async for chunk in self.llm.chat_stream(
                prompt=prompt,
                model=self.model,
                system_message=system_msg,
                temperature=0.7
            ):
                full_content += chunk
                
                yield {
                    'event': 'message_chunk',
                    'content': chunk
                }
            
            # Store in history
            session['history'].append({
                'role': 'assistant',
                'content': full_content,
                'state': session['state'].value
            })
            
            yield {
                'event': 'message_complete',
                'full_content': full_content
            }
        
        except Exception as e:
            logger.error(f"[ThinkGuide] LLM streaming error: {e}", exc_info=True)
            yield {
                'event': 'error',
                'message': str(e)
            }
    
    def get_session(self, session_id: str) -> Optional[Dict]:
        """Get session by ID (for tooltip endpoint)"""
        return self.sessions.get(session_id)


